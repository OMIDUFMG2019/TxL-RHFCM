# -*- coding: utf-8 -*-
"""TxL-RHFCM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TbJeIcOu4fpcJ-apEITCp0VJqr1J1sUf
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import TimeSeriesSplit
from numpy.linalg import svd

# Early Stopping Class
class EarlyStopping:
    def __init__(self, patience=10, verbose=False):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# Positional Encoding Class
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

# Transformer Encoder Block
class TransformerEncoderBlock(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0.3):
        super(TransformerEncoderBlock, self).__init__()
        self.positional_encoding = PositionalEncoding(d_model)
        self.encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)

    def forward(self, src):
        src = self.positional_encoding(src)
        return self.transformer_encoder(src)

# Transformer Decoder Block
class TransformerDecoderBlock(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0.3):
        super(TransformerDecoderBlock, self).__init__()
        self.positional_encoding = PositionalEncoding(d_model)
        self.decoder_layer = nn.TransformerDecoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True
        )
        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=1)

    def forward(self, tgt, memory):
        tgt = self.positional_encoding(tgt)
        return self.transformer_decoder(tgt, memory)

# FuzzyCognitiveMap Class
class FuzzyCognitiveMap(nn.Module):
    def __init__(self, num_concepts, order=1, activation_function=torch.sigmoid):
        super(FuzzyCognitiveMap, self).__init__()
        self.order = order
        self.num_concepts = num_concepts
        self.activation_function = activation_function
        self.weights = nn.ParameterList()
        self.bias = nn.ParameterList()

        for _ in range(self.order):
            weights = nn.Parameter(torch.Tensor(np.random.uniform(-1, 1, (num_concepts, num_concepts))))
            spectral_radius = torch.max(torch.abs(torch.linalg.eigvals(weights)))
            weights.data *= 0.5 / spectral_radius
            self.weights.append(weights)

            bias = nn.Parameter(torch.Tensor(np.random.uniform(-1, 1, num_concepts)))
            U, S, VT = svd(bias.cpu().detach().numpy().reshape(-1, 1))
            spectral_radius = np.max(S)
            bias.data *= 0.5 / spectral_radius
            self.bias.append(bias)

    def forward(self, x):
        batch_size = x.size(0)
        x = x.reshape(batch_size, -1, self.num_concepts)  # Reshape to (batch_size, order, num_concepts)
        for k in range(self.order):
            x = torch.matmul(x, self.weights[k]) + self.bias[k]
            x = self.activation_function(x)
        return x

# XLSTM Class
class xLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout=0.3):
        super(xLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM Gates
        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.cell_gate = nn.Linear(input_size + hidden_size, hidden_size)

        # Extended forget gate
        self.extended_forget_gate = nn.Linear(input_size + hidden_size, hidden_size)

        # Dropout layer
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, hidden):
        h_t, c_t = hidden

        outputs = []
        for t in range(x.size(1)):  # Iterate over time steps
            # Concatenate input and hidden state
            combined = torch.cat((x[:, t], h_t), dim=1)

            # Standard LSTM gates
            forget = torch.sigmoid(self.forget_gate(combined))
            input_ = torch.sigmoid(self.input_gate(combined))
            output = torch.sigmoid(self.output_gate(combined))
            cell_candidate = torch.tanh(self.cell_gate(combined))

            # Extended forget gate
            extended_forget = torch.sigmoid(self.extended_forget_gate(combined))

            # Cell state update with extended forget gate
            c_t = forget * c_t + input_ * cell_candidate
            c_t = extended_forget * c_t  # Incorporating the extended forget gate

            # Hidden state update
            h_t = output * torch.tanh(c_t)

            outputs.append(h_t.unsqueeze(1))

        outputs = torch.cat(outputs, dim=1)  # Concatenate over time steps

        return outputs, (h_t, c_t)

    def init_hidden(self, batch_size):
        h_t = torch.zeros(batch_size, self.hidden_size).to(next(self.parameters()).device)
        c_t = torch.zeros(batch_size, self.hidden_size).to(next(self.parameters()).device)
        return h_t, c_t

# R-HFCM Layer
class RHFCM(nn.Module):
    def __init__(self, num_concepts, order=1, num_fcms=2, activation_function=torch.sigmoid):
        super(RHFCM, self).__init__()
        self.order = order
        self.num_concepts = num_concepts
        self.num_fcms = num_fcms
        self.activation_function = activation_function
        self.fcms = nn.ModuleList([FuzzyCognitiveMap(num_concepts, order, activation_function) for _ in range(num_fcms)])

    def forward(self, x):
        batch_size, sequence_length, num_concepts = x.size()
        assert sequence_length >= self.order, "Sequence length should be greater than or equal to the order"

        fcm_inputs = x[:, -self.order:]  # shape: (batch_size, order, num_concepts)

        activations = []
        for fcm in self.fcms:
            activation = fcm(fcm_inputs)
            activations.append(activation)

        activations = torch.stack(activations, dim=1)
        state_values = torch.mean(activations, dim=1)
        return state_values

# Time Series Model
class TransformerXLSTMRHFCMTimeSeries(nn.Module):
    def __init__(self, input_dim, d_model, nhead, xlstm_hidden_dim, xlstm_layers, output_dim, num_concepts, fcm_order, num_fcms, dropout=0.3):
        super(TransformerXLSTMRHFCMTimeSeries, self).__init__()
        self.encoder = nn.Linear(input_dim, d_model)
        self.transformer_encoder = TransformerEncoderBlock(d_model, nhead, dropout=dropout)
        self.transformer_decoder = TransformerDecoderBlock(d_model, nhead, dropout=dropout)
        self.xlstm = xLSTM(d_model, xlstm_hidden_dim, xlstm_layers, dropout=dropout)  # Use xLSTM here
        self.batch_norm = nn.BatchNorm1d(xlstm_hidden_dim)
        self.rhfcm = RHFCM(xlstm_hidden_dim, order=fcm_order, num_fcms=num_fcms, activation_function=torch.sigmoid)
        self.fc = nn.Linear(xlstm_hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.encoder(x)
        memory = self.transformer_encoder(x)
        transformer_output = self.transformer_decoder(x, memory)

        # Initialize hidden states for xLSTM
        batch_size = x.size(0)
        hidden = self.xlstm.init_hidden(batch_size)

        xlstm_output, hidden = self.xlstm(transformer_output, hidden)  # Use xLSTM forward pass

        xlstm_output = self.batch_norm(xlstm_output.permute(0, 2, 1)).permute(0, 2, 1)
        rhfcm_output = self.rhfcm(xlstm_output)
        context_vector = self.dropout(rhfcm_output.mean(dim=1))  # Mean across the sequence length
        output = self.fc(context_vector)
        return output

# Load and preprocess data
df = pd.read_csv('https://query.data.world/s/e5arbthdytod3m7wfcg7gmtluh3wa5', sep=';')
data = df['load'].values[:8760]

# Normalize data
scaler = StandardScaler()
data = scaler.fit_transform(data.reshape(-1, 1)).squeeze()

# Create sequences
def create_sequences(data, seq_length):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        xs.append(data[i:i + seq_length])
        ys.append(data[i + seq_length])
    return np.array(xs), np.array(ys)

seq_length = 30
X, y = create_sequences(data, seq_length)

# Split data into train and test sets
train_size = int(len(X) * 0.9)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Cross-Validation Setup
n_splits = 5  # Number of folds
tscv = TimeSeriesSplit(n_splits=n_splits)

# Cross-Validation Loop
batch_size = 64
fold = 1
test_results = []
for train_index, val_index in tscv.split(X_train):
    print(f"Fold {fold}/{n_splits}")
    fold += 1

    # Split into training and validation sets
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Create DataLoaders
    train_dataset = TensorDataset(torch.tensor(X_train_fold, dtype=torch.float32).unsqueeze(-1),
                                  torch.tensor(y_train_fold, dtype=torch.float32).unsqueeze(1))
    val_dataset = TensorDataset(torch.tensor(X_val_fold, dtype=torch.float32).unsqueeze(-1),
                                torch.tensor(y_val_fold, dtype=torch.float32).unsqueeze(1))

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # Initialize the model, optimizer, and scheduler for each fold
    input_dim = 1
    d_model = 128
    nhead = 4
    xlstm_hidden_dim = 256
    xlstm_layers = 3
    output_dim = 1
    num_concepts = xlstm_hidden_dim
    fcm_order = 3
    num_fcms = 2
    dropout = 0.3

    model = TransformerXLSTMRHFCMTimeSeries(input_dim, d_model, nhead, xlstm_hidden_dim, xlstm_layers, output_dim, num_concepts, fcm_order, num_fcms, dropout)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    early_stopping = EarlyStopping(patience=10, verbose=True)

    # Train the model for this fold
    epochs = 100
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            output = model(X_batch)
            loss = criterion(output, y_batch)
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping
            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)

        # Validation loss
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                output = model(X_batch)
                loss = criterion(output, y_batch)
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_loader)
        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')

        # Early stopping
        scheduler.step(avg_val_loss)
        early_stopping(avg_val_loss)
        if early_stopping.early_stop:
            print("Early stopping triggered")
            break

    # Evaluate the model on the validation set
    model.eval()
    val_predictions, val_actuals = [], []
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            output = model(X_batch)
            val_predictions.append(output.numpy())
            val_actuals.append(y_batch.numpy())

    # Store validation results for this fold
    val_predictions = scaler.inverse_transform(np.concatenate(val_predictions).squeeze().reshape(-1, 1))
    val_actuals = scaler.inverse_transform(np.concatenate(val_actuals).squeeze().reshape(-1, 1))
    fold_rmse = np.sqrt(mean_squared_error(val_actuals, val_predictions))
    print(f"Fold Validation RMSE: {fold_rmse}")
    test_results.append(fold_rmse)

# Final Cross-Validation Results
print(f"Cross-Validation RMSE Scores: {test_results}")
print(f"Mean RMSE: {np.mean(test_results)}, Std RMSE: {np.std(test_results)}")

# Final Test Evaluation
X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(-1)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

model.eval()
test_predictions, test_actuals = [], []
with torch.no_grad():
    for X_batch, y_batch in test_loader:
        output = model(X_batch)
        test_predictions.append(output.numpy())
        test_actuals.append(y_batch.numpy())

# Convert predictions and actuals back to original scale
test_predictions = scaler.inverse_transform(np.concatenate(test_predictions).squeeze().reshape(-1, 1))
test_actuals = scaler.inverse_transform(np.concatenate(test_actuals).squeeze().reshape(-1, 1))

# Calculate RMSE
test_rmse = np.sqrt(mean_squared_error(test_actuals, test_predictions))
print(f'Test RMSE: {test_rmse}')

# Plot the predictions
plt.figure(figsize=(10, 6))
plt.plot(test_actuals, label='Actual')
plt.plot(test_predictions, label='Predicted')
plt.legend()
plt.title("Predicted vs Actual Load")
plt.xlabel("Time Steps")
plt.ylabel("Load")
plt.show()